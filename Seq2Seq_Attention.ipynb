{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joel-Vijo/Neural-Machine-Translation/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ounqdwK5lCY",
        "outputId": "8218ef0e-c63a-46fd-9168-accfe2e90a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.11.0+cu113)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "Successfully installed sentencepiece-0.1.96 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TL_vtCdt5rbv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import spacy\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.data.metrics import bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW5P6w5v5uOa",
        "outputId": "d7bd8979-12f1-4a7e-a16b-812f0409d4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook is running on cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" )\n",
        "print(\"Notebook is running on\", device)\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBAZSPRb5xdo",
        "outputId": "a006f557-e5c7-4d39-dd0e-a2705e9b953e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.3.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.3.0/de_core_news_sm-3.3.0-py3-none-any.whl (14.6 MB)\n",
            "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download de\n",
        "!python -m spacy download en\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-nbSJTtM81__"
      },
      "outputs": [],
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RVGvldPd845A"
      },
      "outputs": [],
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW4gmFzCnW8H",
        "outputId": "8d29e3cb-d349-4a42-9111-630515ed43e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '.data/multi30k': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls -al .data/multi30k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gXM2p9Df87Nb"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
        "                                                    fields = (SRC, TRG),root=\".data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xh0XhEB189tr"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XEMmA5eT9Au-"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_size,enc_hidden_size,dec_hidden_size,embedding_dim):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embed=nn.Embedding(input_size,embedding_dim)\n",
        "        self.rnn=nn.RNN(embedding_dim,enc_hidden_size,bidirectional=True)\n",
        "        self.h2o=nn.Linear(enc_hidden_size*2,dec_hidden_size)\n",
        "    def forward(self,input,hidden):\n",
        "        input=self.embed(input).to(device)\n",
        "        output,hidden=self.rnn(input)\n",
        "        hidden=torch.tanh(self.h2o(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        return output,hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y-qPjtO3CmIa"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,enc_hidden_size,dec_hidden_size):\n",
        "        super(Attention,self).__init__()\n",
        "        self.att=nn.Linear(enc_hidden_size*2+dec_hidden_size,dec_hidden_size)\n",
        "        self.a2o=nn.Linear(dec_hidden_size,1,bias=False)\n",
        "    def forward(self,encoding_output,hidden):\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, encoding_output.shape[0], 1)\n",
        "        encoding_output = encoding_output.permute(1, 0, 2)\n",
        "        attent_concat=torch.cat((hidden,encoding_output),dim=2)\n",
        "        e=self.att(attent_concat)\n",
        "        output=self.a2o(e)\n",
        "        output=output.squeeze(2)\n",
        "        output=F.softmax(output,dim=1)\n",
        "        return output         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oc3ZmhQjAays"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_size,enc_hidden_size,dec_hidden_size,embedding_dim,attention):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.output_size=output_size\n",
        "        self.attention=attention\n",
        "        self.embed=nn.Embedding(output_size,embedding_dim)\n",
        "        self.rnn=nn.RNN(enc_hidden_size*2+embedding_dim,dec_hidden_size)\n",
        "        self.h2o=nn.Linear(enc_hidden_size*2+embedding_dim+dec_hidden_size,output_size)\n",
        "        \n",
        "    def forward(self,input,hidden,encoder_outputs):\n",
        "        #print(\"Input\",input.size())\n",
        "        input=self.embed(input)\n",
        "        a=self.attention(encoder_outputs,hidden)\n",
        "        a=a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        w=torch.bmm(a,encoder_outputs)\n",
        "        w=w.permute(1,0,2)\n",
        "        hidden=hidden.unsqueeze(0)\n",
        "        output,hidden=self.rnn(torch.cat((w,input),dim=2),hidden)\n",
        "        #print(output.size())\n",
        "        input=input.squeeze(0)\n",
        "        w=w.squeeze(0)\n",
        "        output=output.squeeze(0)\n",
        "        output=self.h2o(torch.cat((output,w,input),dim=1))\n",
        "        #print(\"Output\",output.size())\n",
        "        return output,hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wo9bRj8iJ-34"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.enc=encoder\n",
        "        self.dec=decoder\n",
        "    def forward(self, hidden_size, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size=target.shape[1]\n",
        "        target_length=target.shape[0]\n",
        "        vocab_size=self.dec.output_size\n",
        "        outputs=torch.zeros(target_length,batch_size,vocab_size).to(device)\n",
        "        inputs=target[0,:]\n",
        "        inputs=inputs.unsqueeze(0)\n",
        "        hidden=(torch.zeros((1, batch_size, hidden_size)),torch.zeros((1, batch_size, hidden_size)))\n",
        "        encoder_outputs,hidden=self.enc(source,hidden)\n",
        "        #print(hidden.size())\n",
        "        for i in range(1,target_length):\n",
        "          output,hidden=self.dec(inputs,hidden,encoder_outputs)\n",
        "          #print(\"Done\")\n",
        "          #print(hidden.size())\n",
        "          hidden=hidden.squeeze(0)\n",
        "          #print(hidden.size())\n",
        "          outputs[i]=output\n",
        "          teacher_force=random.random()<teacher_forcing_ratio\n",
        "          if(teacher_force):\n",
        "            inputs=target[i]\n",
        "            #print(\"Input1 \",inputs.size())\n",
        "            inputs=inputs.unsqueeze(0)\n",
        "          else:\n",
        "            inputs=output.argmax(1)\n",
        "            #print(\"Input2\",inputs.size())\n",
        "            inputs=inputs.unsqueeze(0)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJsVau58fGa0",
        "outputId": "6c8d2d3d-7f6a-4d24-8e9f-0bc09bc77f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5893\n",
            "The model has 17,104,389 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "i_size=len(SRC.vocab)\n",
        "o_size=len(TRG.vocab)\n",
        "e_embed_dim=256\n",
        "d_embed_dim=256\n",
        "e_h_size=512\n",
        "d_h_size=512\n",
        "learning_rate=0.001\n",
        "enc=Encoder(i_size,e_h_size,d_h_size,e_embed_dim)\n",
        "att=Attention(e_h_size,d_h_size)\n",
        "dec=Decoder(o_size,e_h_size,d_h_size,d_embed_dim,att)\n",
        "model=Seq2Seq(enc,dec).to(device)\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),learning_rate)\n",
        "BATCH_SIZE = 128\n",
        "print(o_size)\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,device=device)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AxyLMsaOh5vy"
      },
      "outputs": [],
      "source": [
        "def train(model,hidden_size,iterator,criterion,optimiser):\n",
        "  model.train()\n",
        "  epoch_loss=0\n",
        "  for i,batch in enumerate(iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg    \n",
        "    optimizer.zero_grad()\n",
        "    output = model(hidden_size, src, trg)\n",
        "    o_size=output.shape[-1]\n",
        "    output = output[1:].view(-1, o_size)\n",
        "    trg = trg[1:].view(-1)\n",
        "    loss = criterion(output, trg)   \n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  return epoch_loss/len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_nM0sY5iAXY",
        "outputId": "56ed4a38-f32a-4205-f892-7848ee7a756b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 4.424807555875064\n",
            "Training 3.34727902139336\n",
            "Training 2.8558163180750373\n",
            "Training 2.524192287008143\n",
            "Training 2.2898680696403404\n",
            "Training 2.0754465788996694\n",
            "Training 1.8771815599323896\n",
            "Training 1.7876382399235529\n",
            "Training 1.6522211041219435\n",
            "Training 1.5386886980039958\n",
            "Training 1.414779695645303\n",
            "Training 1.3240941334926084\n",
            "Training 1.2548796802365307\n",
            "Training 1.1723217155439738\n",
            "Training 1.092545553188492\n",
            "Training 1.034842373516066\n",
            "Training 0.9628520794376928\n",
            "Training 0.8869004344099943\n",
            "Training 0.8131342992383478\n",
            "Training 0.7540306813916445\n"
          ]
        }
      ],
      "source": [
        "for i in range(20):\n",
        "  train_loss=train(model,e_h_size,train_iterator,criterion,optimizer)\n",
        "  print(\"Training\",train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ugXWLFmBuSBt"
      },
      "outputs": [],
      "source": [
        "def ipTensor(sentence, src_field):\n",
        "    if isinstance(sentence, list):\n",
        "        tokens = [src_field.init_token] + [token.lower() for token in sentence] + [src_field.eos_token]\n",
        "    else:\n",
        "        tokens = [src_field.init_token] + tokenize_de(sentence) + [src_field.eos_token]\n",
        "    seq_len = len(tokens)\n",
        "    ip_tensor = torch.LongTensor([src_field.vocab.stoi[token] for token in tokens]).to(device)\n",
        "    return ip_tensor.view(seq_len, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KMZYv5yLuVAf"
      },
      "outputs": [],
      "source": [
        "def Evaluate(iterator, model, criterion):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            model.zero_grad()\n",
        "            source = batch.src\n",
        "            target = batch.trg\n",
        "            print(target)\n",
        "            outputs = model(e_h_size,source, target)\n",
        "            outputs = outputs[1:].view(-1, o_size)\n",
        "            targets = target[1:].view(-1)\n",
        "            batch_loss = criterion(outputs, targets)\n",
        "            eval_loss += batch_loss.item()\n",
        "        \n",
        "        return eval_loss/len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7vDZBxyuV39",
        "outputId": "eda9d3a6-1600-4dec-cc34-aee22a0d43c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [  16,  110,    4,  ...,    4,   24,   16],\n",
            "        [1909,   19,   34,  ...,   14,   14,   30],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\n",
            "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
            "        [ 14,   4,   4,  ...,   4,  63, 795],\n",
            "        [395,  35,   9,  ...,  14, 270,   6],\n",
            "        ...,\n",
            "        [  3,   3,   3,  ...,   3,   3,   3],\n",
            "        [  1,   1,   1,  ...,   1,   1,   1],\n",
            "        [  1,   1,   1,  ...,   1,   1,   1]], device='cuda:0')\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [   4,    4,   63,  ...,    7,   25,    7],\n",
            "        [   9,   38,  929,  ...,   34, 1547, 2222],\n",
            "        ...,\n",
            "        [   5,    5,    5,  ...,    1,    1,    1],\n",
            "        [   3,    3,    3,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [  70,   16,    4,  ...,  110,   63,   19],\n",
            "        [1202,   30,   24,  ...,  104,  281,   17],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [ 120,    4,   21,  ...,    4,   21,    4],\n",
            "        [  13, 1398,  324,  ...,   35,  145,  974],\n",
            "        ...,\n",
            "        [2235,  206,   98,  ...,    1,    1,    1],\n",
            "        [   5,    5,    5,  ...,    1,    1,    1],\n",
            "        [   3,    3,    3,  ...,    1,    1,    1]], device='cuda:0')\n",
            "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
            "        [  4,   7,   4,  ...,   4,   4, 157],\n",
            "        [ 14, 123,   9,  ..., 348,   9, 467],\n",
            "        ...,\n",
            "        [882, 105, 954,  ...,   1,   1,   3],\n",
            "        [  5,   5,   5,  ...,   1,   1,   1],\n",
            "        [  3,   3,   3,  ...,   1,   1,   1]], device='cuda:0')\n",
            "tensor([[ 2,  2,  2,  ...,  2,  2,  2],\n",
            "        [ 4,  4,  4,  ...,  4,  4, 16],\n",
            "        [14,  9, 24,  ..., 64, 64, 24],\n",
            "        ...,\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1]], device='cuda:0')\n",
            "tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
            "        [  4,   4,   4,  ...,   7,   4,   4],\n",
            "        [  9,   9,  34,  ...,  16,   9, 183],\n",
            "        ...,\n",
            "        [ 98,  98,   1,  ...,   1,   1,   1],\n",
            "        [  5,   5,   1,  ...,   1,   1,   1],\n",
            "        [  3,   3,   1,  ...,   1,   1,   1]], device='cuda:0')\n",
            "3.478732079267502\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_loss = Evaluate(test_iterator, model, criterion)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ugT6E1pmuYgI"
      },
      "outputs": [],
      "source": [
        "def Translate(src_sentence, src_field, trg_field, model):\n",
        "    ip_tensor = ipTensor(src_sentence, src_field)\n",
        "    max_len = 4*ip_tensor.shape[0]\n",
        "    src_len = [ip_tensor.shape[0]]\n",
        "    hidden=(torch.zeros((1, 1, e_h_size)),torch.zeros((1, 1, e_h_size)))\n",
        "    with torch.no_grad():\n",
        "        enc_outputs,enc_states = model.enc(ip_tensor,hidden)\n",
        "    dec_states = enc_states\n",
        "    sos_id = trg_field.vocab.stoi[trg_field.init_token]\n",
        "    eos_id = trg_field.vocab.stoi[trg_field.eos_token]\n",
        "    predicts = [sos_id]\n",
        "    length = 1\n",
        "    while length < max_len:\n",
        "        input = torch.LongTensor([predicts[-1]]).view((1, 1)).to(device)\n",
        "        with torch.no_grad():\n",
        "            output,dec_states = model.dec(input, dec_states, enc_outputs)\n",
        "        dec_states=dec_states.squeeze(0)\n",
        "        output = output.squeeze()\n",
        "        output = output.view(-1, model.dec.output_size)\n",
        "        predicts.append(output.argmax(-1).item())\n",
        "        length += 1\n",
        "        if predicts[-1] == eos_id:\n",
        "            break\n",
        "    sentence = [trg_field.vocab.itos[id] for id in predicts[1:]]\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WzaMTnbucIi",
        "outputId": "da600b6a-f02c-4847-c8ba-740421f874a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "German:  marathonläuferinnen laufen auf einer städtischen straße während andere personen um sie herum stehen .\n",
            "English:  firetrucks are around around a street around around while other people around around .\n",
            "Actual Translation:  marathon runners are racing on a city street , with other people standing around .\n"
          ]
        }
      ],
      "source": [
        "ind = int(random.random() * len(test_data.examples))\n",
        "example = test_data.examples[ind]\n",
        "src_sentence = example.src\n",
        "trg_sentence = example.trg\n",
        "print(\"German: \", ' '.join(src_sentence))\n",
        "translation = Translate(src_sentence, SRC, TRG, model)\n",
        "print(\"English: \", ' '.join(translation[:-1]))\n",
        "print(\"Actual Translation: \", ' '.join(trg_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Vl-clzU66Rww"
      },
      "outputs": [],
      "source": [
        "def Calculate_BLEU(data, src_field, trg_field, model):\n",
        "    trgs = []\n",
        "    predicted_trgs = []\n",
        "    for i in range(len(data.examples)):\n",
        "        src_sentence = vars(data[i])['src']\n",
        "        trg_sentence = vars(data[i])['trg']\n",
        "        try:                               \n",
        "            predicted_trg = Translate(src_sentence, src_field, trg_field, model)\n",
        "            predicted_trgs.append(predicted_trg[:-1])\n",
        "            trgs.append([trg_sentence])\n",
        "        except:\n",
        "            pass\n",
        "    return bleu_score(predicted_trgs, trgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMhcqQPY6Tmr",
        "outputId": "ad2b3c6c-77a4-4a2d-d6d1-fad103b8b61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score on Testing Data: 21.95\n"
          ]
        }
      ],
      "source": [
        "bleu_score_test = Calculate_BLEU(test_data, SRC, TRG, model)\n",
        "print(f\"BLEU score on Testing Data: {bleu_score_test*100:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZ1f9iGcm8+ireNBl7QtHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
